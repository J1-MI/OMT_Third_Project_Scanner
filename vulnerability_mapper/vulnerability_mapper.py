import os
import json
import sqlite3
import logging
import asyncio
import aiohttp
import requests
from configparser import ConfigParser
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Set

# --- 1. ì„¤ì • ë° ë¡œê¹… ì´ˆê¸°í™” ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(module)s - %(message)s')

def load_config(config_path='config.ini'):
    """ì„¤ì • íŒŒì¼ì„ ë¡œë“œí•˜ê³  ëª¨ë“  í•„ìš”í•œ í‚¤ê°€ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤."""
    config = ConfigParser()
    if not os.path.exists(config_path):
        logging.critical(f"ì„¤ì • íŒŒì¼ '{config_path}'ê°€ ì—†ìŠµë‹ˆë‹¤. ì¢…ë£Œí•©ë‹ˆë‹¤.")
        exit()
    config.read(config_path, encoding='utf-8')
    try:
        cfg = {
            'nvd_api_key': config.get('API', 'NVD_API_KEY'),
            'shodan_api_key': config.get('API', 'SHODAN_API_KEY', fallback=None),
            'github_api_key': config.get('API', 'GITHUB_API_KEY', fallback=None),
            'otx_api_key': config.get('API', 'OTX_API_KEY', fallback=None),
            'cvss_weight': config.getfloat('SCORING', 'CVSS_WEIGHT'),
            'epss_weight': config.getfloat('SCORING', 'EPSS_WEIGHT'),
            'exploit_bonus': config.getfloat('SCORING', 'PUBLIC_EXPLOIT_BONUS'),
            'threat_bonus': config.getfloat('SCORING', 'THREAT_INTEL_BONUS'),
            'temporal_bonus': config.getfloat('SCORING', 'TEMPORAL_WEIGHT_BONUS'),
            'db_file': config.get('DATABASE', 'DB_FILE'),
            'cache_ttl': config.getint('CACHE', 'TTL')
        }

        # â–¼â–¼â–¼â–¼â–¼ ë””ë²„ê¹…ì„ ìœ„í•œ ì½”ë“œ ì¶”ê°€ â–¼â–¼â–¼â–¼â–¼
        print("-" * 50)
        print(f"DEBUG: í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ NVD API í‚¤ëŠ” [ {cfg['nvd_api_key']} ] ì…ë‹ˆë‹¤.")
        print("-" * 50)
        # â–²â–²â–²â–²â–² ë””ë²„ê¹… ì½”ë“œ ì¢…ë£Œ â–²â–²â–²â–²â–²

        if 'YOUR_NVD_API_KEY' in cfg['nvd_api_key']:
            logging.warning("config.iniì˜ NVD_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return cfg
    except Exception as e:
        logging.critical(f"config.ini ë¡œë“œ ì‹¤íŒ¨: {e}")
        exit()

CONFIG = load_config()

# --- 2. CISA KEV ì¹´íƒˆë¡œê·¸ ë¡œë” ---
def load_kev_catalog() -> Set[str]:
    """CISA KEV JSON ì¹´íƒˆë¡œê·¸ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì—¬ CVE ID ì„¸íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    url = "https://www.cisa.gov/sites/default/files/feeds/known_exploited_vulnerabilities.json"
    try:
        response = requests.get(url, timeout=15)
        response.raise_for_status()
        data = response.json()
        kev_set = {vuln['cveID'] for vuln in data.get('vulnerabilities', [])}
        logging.info(f"CISA KEV ì¹´íƒˆë¡œê·¸ ë¡œë“œ ì™„ë£Œ: {len(kev_set)}ê°œì˜ ìœ„í˜‘ í™•ì¸.")
        return kev_set
    except Exception as e:
        logging.error(f"CISA KEV ì¹´íƒˆë¡œê·¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return set()

# --- 3. ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬ ---
def initialize_database(db_file: str):
    """ëª¨ë“ˆì´ ì‚¬ìš©í•  DB(ìºì‹œ, ë¡œê·¸ìš©)ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    os.makedirs(os.path.dirname(db_file), exist_ok=True)
    with sqlite3.connect(db_file) as conn:
        cursor = conn.cursor()
        cursor.execute('''CREATE TABLE IF NOT EXISTS cve_cache (key TEXT PRIMARY KEY, data TEXT NOT NULL, cached_at DATETIME NOT NULL)''')
        cursor.execute('''CREATE TABLE IF NOT EXISTS scan_logs (id INTEGER PRIMARY KEY AUTOINCREMENT, timestamp DATETIME NOT NULL, input_data TEXT NOT NULL, cve_id TEXT NOT NULL, risk_score REAL NOT NULL, risk_rating TEXT NOT NULL)''')
        conn.commit()

# --- 4. ë°ì´í„° ëª¨ë¸ ë° íŒŒì„œ ---
@dataclass
class ScannedService:
    ip: str; port: int; service_name: str; service_version: str

def parse_scan_results(file_path: str) -> List[ScannedService]:
    """ì…ë ¥ íŒŒì¼ì˜ í™•ì¥ìë¥¼ í™•ì¸í•˜ì—¬ JSON ë˜ëŠ” Nmap XMLì„ ìë™ìœ¼ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤."""
    services = []
    try:
        if file_path.endswith('.json'):
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            required_keys = ['ip', 'port', 'service_name', 'service_version']
            for item in data:
                if all(key in item for key in required_keys):
                    services.append(ScannedService(**item))
                else:
                    logging.warning(f"JSON í•„ìˆ˜ í‚¤ ëˆ„ë½, ê±´ë„ˆëœë‹ˆë‹¤: {item}")

        elif file_path.endswith('.xml'):
            tree = etree.parse(file_path)
            root = tree.getroot()
            for host in root.findall('host'):
                ip_address = host.find('address').get('addr')
                for port in host.findall('ports/port'):
                    if port.find('state').get('state') == 'open':
                        port_num = int(port.get('portid'))
                        service_node = port.find('service')
                        service_name = service_node.get('product', service_node.get('name', ''))
                        service_version = service_node.get('version', '')
                        
                        services.append(ScannedService(
                            ip=ip_address,
                            port=port_num,
                            service_name=service_name,
                            service_version=service_version
                        ))
        else:
            logging.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {file_path}")

        logging.info(f"'{file_path}'ì—ì„œ {len(services)}ê°œì˜ ìœ íš¨í•œ ì„œë¹„ìŠ¤ ì •ë³´ë¥¼ íŒŒì‹±í–ˆìŠµë‹ˆë‹¤.")
        return services
        
    except Exception as e:
        logging.error(f"ìŠ¤ìº” ê²°ê³¼ íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return []

# --- 5. í•µì‹¬ ë¶„ì„ ì—”ì§„ (6ê°œ API ì—°ë™) ---
async def get_from_cache_or_api(session: aiohttp.ClientSession, key: str, api_call_coro):
    """ìºì‹œë¥¼ í™•ì¸í•˜ê³ , ì—†ìœ¼ë©´ APIë¥¼ í˜¸ì¶œí•˜ì—¬ ê²°ê³¼ë¥¼ ìºì‹±í•©ë‹ˆë‹¤."""
    with sqlite3.connect(CONFIG['db_file']) as conn:
        cursor = conn.cursor()
        result = cursor.execute("SELECT data, cached_at FROM cve_cache WHERE key = ?", (key,)).fetchone()
        if result and datetime.now() - datetime.fromisoformat(result[1]) < timedelta(seconds=CONFIG['cache_ttl']):
            return json.loads(result[0])
    
    try:
        data = await api_call_coro(session, key)
        if data is not None:
            with sqlite3.connect(CONFIG['db_file']) as conn:
                cursor = conn.cursor()
                cursor.execute("REPLACE INTO cve_cache (key, data, cached_at) VALUES (?, ?, ?)",
                               (key, json.dumps(data), datetime.now().isoformat()))
        return data
    except Exception as e:
        logging.warning(f"{key}ì— ëŒ€í•œ API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return None

async def _api_get_cve_details(session: aiohttp.ClientSession, cve_id: str):
    headers = {'apiKey': CONFIG['nvd_api_key']}
    async with asyncio.TaskGroup() as tg:
        nvd_task = tg.create_task(session.get(f"https://services.nvd.nist.gov/rest/json/cves/2.0?cveId={cve_id}", headers=headers))
        epss_task = tg.create_task(session.get(f"https://api.first.org/data/v1/epss?cve={cve_id}"))
    nvd_res, epss_res = nvd_task.result(), epss_task.result()
    nvd_data = (await nvd_res.json())['vulnerabilities'][0]['cve']
    epss_data = (await epss_res.json()).get("data", [])
    return {
        "cvss_score": nvd_data.get('metrics', {}).get('cvssMetricV31', [{}])[0].get('cvssData', {}).get('baseScore', 0.0),
        "epss_percentile": float(epss_data[0].get('epss', 0.0)) * 100 if epss_data else 0.0,
        "description": nvd_data['descriptions'][0]['value'],
        "published_date": nvd_data.get('published')
    }

async def _api_check_github_poc(session: aiohttp.ClientSession, cve_id: str):
    url = f"https://api.github.com/search/repositories?q={cve_id}+exploit"
    headers = {"Authorization": f"token {CONFIG['github_api_key']}"} if CONFIG['github_api_key'] else {}
    async with session.get(url, headers=headers) as res:
        return (await res.json()).get('total_count', 0) > 0

async def _api_get_otx_context(session: aiohttp.ClientSession, cve_id: str):
    if not CONFIG.get('otx_api_key'): return []
    url = f"https://otx.alienvault.com/api/v1/indicators/cve/{cve_id}/general"
    headers = {"X-OTX-API-KEY": CONFIG['otx_api_key']}
    keywords = []
    async with session.get(url, headers=headers) as res:
        if res.status == 200:
            pulses = (await res.json()).get('pulse_info', {}).get('pulses', [])
            for pulse in pulses:
                name_lower = pulse.get('name', '').lower()
                if 'ransomware' in name_lower: keywords.append("Ransomware")
                if 'malware' in name_lower: keywords.append("Malware")
    return list(set(keywords))

def calculate_risk_score(details: dict, context: dict, asset_importance: float, kev_catalog: set) -> tuple[float, str]:
    cve_id = details.get('cve_id', '')
    if cve_id in kev_catalog:
        return 10.0, "ğŸš¨ URGENT (Actively Exploited)"

    cvss, epss = details.get('cvss_score', 0.0), details.get('epss_percentile', 0.0)
    score = (cvss * CONFIG['cvss_weight']) + ((epss / 10) * CONFIG['epss_weight'])
    
    if context.get('has_public_exploit'): score += CONFIG['exploit_bonus']
    if context.get('threat_keywords'): score += CONFIG['threat_bonus']
    
    temporal_bonus = 0.0
    if details.get('published_date'):
        published = datetime.fromisoformat(details['published_date'].replace('Z', '+00:00'))
        if datetime.now(published.tzinfo) - published < timedelta(days=90):
            temporal_bonus = CONFIG['temporal_bonus']
    
    score = score * asset_importance + temporal_bonus
    score = min(10.0, round(score, 2))

    if score >= 9.0: rating = "Critical"
    elif score >= 7.0: rating = "High"
    elif score >= 4.0: rating = "Medium"
    else: rating = "Low"
    return score, rating

async def get_shodan_context(session: aiohttp.ClientSession, ip: str) -> Dict:
    if not CONFIG.get('shodan_api_key'): return {}
    url = f"https://api.shodan.io/shodan/host/{ip}?key={CONFIG['shodan_api_key']}"
    try:
        async with session.get(url) as res:
            return await res.json() if res.status == 200 else {}
    except Exception as e:
        logging.warning(f"Shodan ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨ (IP: {ip}): {e}")
        return {}

# 6. ë©”ì¸ ì»¨íŠ¸ë¡¤ëŸ¬
async def analyze_services(services: List[ScannedService], asset_importance_map: Dict[str, float] = {}, kev_catalog: Set[str] = set()) -> Dict[str, Any]:
    final_results = {}
    async with aiohttp.ClientSession() as session:
        for service in services:
            logging.info(f"ë¶„ì„ ì‹œì‘: {service.service_name} on {service.ip}:{service.port}")
            shodan_task = asyncio.create_task(get_shodan_context(session, service.ip))
            
            try:
                params = {'keywordSearch': f"{service.service_name} {service.service_version}", 'resultsPerPage': 20}
                async with session.get("https://services.nvd.nist.gov/rest/json/cves/2.0", headers={'apiKey': CONFIG['nvd_api_key']}, params=params) as res:
                    res.raise_for_status()
                    found_cves = [vuln['cve']['id'] for vuln in (await res.json()).get('vulnerabilities', [])]
            except Exception as e:
                logging.error(f"'{service.service_name}' CVE ê²€ìƒ‰ ì‹¤íŒ¨: {e}"); continue
            
            if not found_cves:
                shodan_info = await shodan_task
                final_results[f"{service.ip}:{service.port}"] = {"service": asdict(service), "shodan_info": shodan_info, "vulnerabilities": []}
                continue

            tasks = {}
            for cve_id in found_cves:
                tasks[cve_id] = {
                    "details": asyncio.create_task(get_from_cache_or_api(session, cve_id, _api_get_cve_details)),
                    "exploit": asyncio.create_task(get_from_cache_or_api(session, f"gh_{cve_id}", _api_check_github_poc)),
                    "otx": asyncio.create_task(get_from_cache_or_api(session, f"otx_{cve_id}", _api_get_otx_context)),
                }

            analyzed_vulns = []
            for cve_id, task_group in tasks.items():
                details = await task_group['details']
                if not details: continue
                details['cve_id'] = cve_id
                
                context = {"has_public_exploit": await task_group['exploit'], "threat_keywords": await task_group['otx']}
                risk_score, risk_rating = calculate_risk_score(details, context, asset_importance_map.get(service.ip, 1.0), kev_catalog)
                
                with sqlite3.connect(CONFIG['db_file']) as conn:
                    conn.cursor().execute("INSERT INTO scan_logs VALUES (NULL, ?, ?, ?, ?, ?)", (datetime.now(), json.dumps(asdict(service)), cve_id, risk_score, risk_rating))
                    conn.commit()

                analyzed_vulns.append({"risk_score": risk_score, "risk_rating": risk_rating, **details, "context": context})

            shodan_info = await shodan_task
            final_results[f"{service.ip}:{service.port}"] = {
                "service": asdict(service),
                "shodan_info": shodan_info,
                "vulnerabilities": sorted(analyzed_vulns, key=lambda x: x['risk_score'], reverse=True)
            }
    return final_results

# 7. ëª¨ë“ˆ ë…ë¦½ ì‹¤í–‰ í…ŒìŠ¤íŠ¸
async def main():
    initialize_database(CONFIG['db_file'])
    kev_catalog = load_kev_catalog()
    asset_map = {"192.168.1.10": 1.5, "192.168.1.20": 0.8}
    services_to_scan = parse_scan_results("scan_results.json")
    
    if services_to_scan:
        final_report = await analyze_services(services_to_scan, asset_map, kev_catalog)
        print("\n--- ìµœì¢… ë¶„ì„ ê²°ê³¼ (JSON ì¶œë ¥) ---")
        print(json.dumps(final_report, indent=2, ensure_ascii=False))

if __name__ == '__main__':
    # ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜: pip install requests aiohttp
    # ì‹¤í–‰ ì „ config.ini íŒŒì¼ì— API í‚¤ë“¤ì„ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.
    asyncio.run(main())